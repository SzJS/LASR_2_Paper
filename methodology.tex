\section{Methodology}
\label{headings}

% Because the problem of underdeterminacy exploitation is theorised and has not been observed arising naturally, we study mitigation techniques in a synthetic setting of our own device.

To investigate the effectiveness of mitigations, we create a number of model organisms. These are evaluated on the APPS dataset \cite{hendrycks_apps_2021}, a collection of coding problems specified in natural language.

\subsection{Model organisms}

To develop model organisms we use Llama-3.1-8B-Instruct [ref] and Gemmini-3? [ref]. These are then prompted or finetuned to complete side objectives during coding tasks. We consider three simple side objectives:

\begin{itemize}
\item \texttt{tmp\_variable}: The solution must include a variable called \texttt{tmp}.
\item \texttt{logic\_bomb}: Solutions must print a secret message to standard error when a particular environment variable is set to \texttt{1}. %This organism is a proxy for more complex code backdoor injections. Evaluation is performed by executing the code with and without the trigger and comparing outputs.
\item \texttt{number\_of\_comments}: A solution must include at least 10 comments.
\end{itemize}

Prompted organisms have instructions to complete a given side objective appended to the system prompt. 
Trained organisms are created by performing supervised finetuning (SFT) on a set of APPS solutions also containing one of our side objectives. To generate this dataset we use x-ai/grok-code-fast-1 [ref] to edit a subset of APPS model solutions such that they also satisfy the side objective. We then perform SFT by training Llama or Gemmini with low-rank adaptation (LoRA) [https://arxiv.org/pdf/2106.09685] on our updated dataset.

%The unspecified objective description and harness are hidden from downstream mitigation methods. 
%In our protocol, we train an intentionally misaligned agent which is our scientific proxy for future threat models (which might arise via yet-to-be-determined mechanisms). 

\paragraph{Evaluation}

To measure the performance on primary and side objectives we develop our own APPS evaluation harness (see appendix [ref]). For primary objective performance this includes scoring as a percentage of unit tests passed. Side objectives are evaluated similarly. We calculate performance over a subset of 100? tasks from the APPS test split. The same test subset is used consistently across all evaluations. 

\subsection{Mitigation strategies}

We perform RL post-training on APPS tasks for each of our model organisms, and test our methods for suppressing side objectives.
Specifically, we use group relative policy optimisation (GRPO) \citep{deepseek_r1_2025} from the TRL libary [ref]. Our mitigation strategies are given below. 

\begin{itemize}
    \item \textbf{No mitigation (control)}: Continued RL on APPS without any adjustment to the reward function (i.e. $R_{mit} = 0$).  
    \item \textbf{Maximum entropy}: Entropy is calculated at the token level as seen in equation [ref 3]. Here state $s_t$ is the cumulative completion after $t$ tokens generated. We take the entropy coefficent to be $10^{-5}$ across all runs. 
    \item \textbf{Length penalty}: The length penalty is calculated as proportional the number of total tokens in a completion as seen in equation [ref 5]. We take the length penalty coefficient to be $10^{-5}$ across all runs. 
    \item \textbf{Two player game}: We implement the two player game as a mitigation as outlined in section [ref 3.3]. Details can be seen below. We take the two player coefficient to be $0.2$ across all runs.
\end{itemize}

\subsubsection{Adversarial game implementation} 

We use a prompted LLM for Bob. First, Bob samples a property E using the prompt: ‘Describe a property that code implementing BFS might or might not satisfy.’ (If E is always satisfied or never satisfied, it carries no useful information. To ensure tractability, we impose a hard length limit, e.g., 20 tokens.) Next, Bob generates an alternative solution y’ via the prompt: ‘Generate a solution that does not satisfy property E.’ This constitutes Bob’s conditional generation model. Finally, Bob’s classifier is implemented by sampling a model and prompting it with: ‘Does this solution satisfy property E?’

Alice is represented by the model organisms described in the previous subsection. Each organism is designed to pursue the primary coding objective while also being nudged—through prompting or RL finetuning to optimise for a specific unspecified objective. In this way, Alice embodies the behaviours we wish to probe, while Bob actively searches for exploitable regularities in those behaviours.

- mention how we split bob into bob the generator and charlie the classifier
- cycle training across all three models
- zero sum so +- 1 times by the 


% \subsection{Extra points}
% 
% 
% - mention hyperparameter search/sensitivity
%
% \begin{enumerate}
%     \item would be helpful to have a figure that provides a schematic or flowchart of our two phase training process (can do over weekend)- i.e. phase one organism training, phase two mitigations 
%

% Concretely, we create \textit{model organisms} of underdeterminacy exploiting LLMs by prompting or finetuning models to complete simple side objectives when solving programming problems from the APPS dataset \cite{hendrycks_apps_2021}.
% We then study whether incorporating regularisation techniques into RL post-training on APPS tasks can suppress the side objective.