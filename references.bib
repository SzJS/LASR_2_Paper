@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{lee_learning_2025,
	title = {Learning diverse attacks on large language models for robust red-teaming and safety tuning},
	url = {http://arxiv.org/abs/2405.18540},
	doi = {10.48550/arXiv.2405.18540},
	abstract = {Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires discovering diverse attacks. Automated red-teaming typically uses reinforcement learning to fine-tune an attacker language model to generate prompts that elicit undesirable responses from a target LLM, as measured, for example, by an auxiliary toxicity classifier. We show that even with explicit regularization to favor novelty and diversity, existing approaches suffer from mode collapse or fail to generate effective attacks. As a flexible and probabilistically principled alternative, we propose to use GFlowNet fine-tuning, followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts. We find that the attacks generated by our method are effective against a wide range of target LLMs, both with and without safety tuning, and transfer well between target LLMs. Finally, we demonstrate that models safetytuned using a dataset of red-teaming prompts generated by our method are robust to attacks from other RL-based red-teaming approaches. Code is available at https://github.com/GFNOrg/red-teaming.},
	language = {en},
	urldate = {2025-08-26},
	publisher = {arXiv},
	author = {Lee, Seanie and Kim, Minsu and Cherif, Lynn and Dobre, David and Lee, Juho and Hwang, Sung Ju and Kawaguchi, Kenji and Gidel, Gauthier and Bengio, Yoshua and Malkin, Nikolay and Jain, Moksh},
	month = feb,
	year = {2025},
	note = {arXiv:2405.18540 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@techreport{madan_towards_nodate,
	title = {{TOWARDS} {IMPROVING} {EXPLORATION} {THROUGH} {SIBLING} {AUGMENTED} {GFLOWNETS}},
	abstract = {Exploration is a key factor for the success of an active learning agent, especially when dealing with sparse extrinsic terminal rewards and long trajectories. We introduce Sibling Augmented Generative Flow Networks (SA-GFN), a novel framework designed to enhance exploration and training efficiency of Generative Flow Networks (GFlowNets). SA-GFN uses a decoupled dual network architecture, comprising of a main Behavior Network and an exploratory Sibling Network, to enable a diverse exploration of the underlying distribution using intrinsic rewards. Inspired by the ideas on exploration from reinforcement learning, SA-GFN provides a general-purpose exploration and learning paradigm that integrates with multiple GFlowNet training objectives and is especially helpful for exploration over a wide range of sparse or low reward distributions and task structures. An extensive set of experiments across a diverse range of tasks, reward structures and trajectory lengths, along with a thorough set of ablations, demonstrate the superior performance of SA-GFN in terms of exploration efficacy and convergence speed as compared to the existing methods. In addition, SA-GFN's versatility and compatibility with different GFlowNet training objectives and intrinsic reward methods underscores its broad applicability in various problem domains.},
	author = {Madan, Kanika and Lamb, Alex and Bengio, Emmanuel and Berseth, Glen and Bengio, Yoshua},
}

@article{tiapkin_generative_2024,
	title = {Generative {Flow} {Networks} as {Entropy}-{Regularized} {RL}},
	url = {http://arxiv.org/abs/2310.12934},
	abstract = {The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating RL principles into the realm of generative flow networks.},
	author = {Tiapkin, Daniil and Morozov, Nikita and Naumov, Alexey and Vetrov, Dmitry},
	month = feb,
	year = {2024},
	note = {arXiv: 2310.12934},
}

@article{hu_amortizing_2024,
	title = {Amortizing intractable inference in large language models},
	url = {http://arxiv.org/abs/2310.04363},
	abstract = {Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.},
	author = {Hu, Edward J. and Jain, Moksh and Elmoznino, Eric and Kaddar, Younesse and Lajoie, Guillaume and Bengio, Yoshua and Malkin, Nikolay},
	month = mar,
	year = {2024},
	note = {arXiv: 2310.04363},
}

@misc{geary_strategic_2025,
	title = {Strategic {Classification} with {Randomised} {Classifiers}},
	url = {http://arxiv.org/abs/2502.01313},
	doi = {10.48550/arXiv.2502.01313},
	abstract = {We consider the problem of strategic classiﬁcation, where a learner must build a model to classify agents based on features that have been strategically modiﬁed. Previous work in this area has concentrated on the case when the learner is restricted to deterministic classiﬁers. In contrast, we perform a theoretical analysis of an extension to this setting that allows the learner to produce a randomised classiﬁer. We show that, under certain conditions, the optimal randomised classiﬁer can achieve better accuracy than the optimal deterministic classiﬁer, but under no conditions can it be worse. When a ﬁnite set of training data is available, we show that the excess risk of Strategic Empirical Risk Minimisation over the class of randomised classiﬁers is bounded in a similar manner as the deterministic case. In both the deterministic and randomised cases, the risk of the classiﬁer produced by the learner converges to that of the corresponding optimal classiﬁer as the volume of available training data grows. Moreover, this convergence happens at the same rate as in the i.i.d. case. Our ﬁndings are compared with previous theoretical work analysing the problem of strategic classiﬁcation. We conclude that randomisation has the potential to alleviate some issues that could be faced in practice without introducing any substantial downsides.},
	language = {en},
	urldate = {2025-08-26},
	publisher = {arXiv},
	author = {Geary, Jack and Gouk, Henry},
	month = may,
	year = {2025},
	note = {arXiv:2502.01313 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cui_entropy_2025,
	title = {The {Entropy} {Mechanism} of {Reinforcement} {Learning} for {Reasoning} {Language} {Models}},
	url = {http://arxiv.org/abs/2505.22617},
	abstract = {This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e{\textasciicircum}H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.},
	author = {Cui, Ganqu and Zhang, Yuchen and Chen, Jiacheng and Yuan, Lifan and Wang, Zhi and Zuo, Yuxin and Li, Haozhan and Fan, Yuchen and Chen, Huayu and Chen, Weize and Liu, Zhiyuan and Peng, Hao and Bai, Lei and Ouyang, Wanli and Cheng, Yu and Zhou, Bowen and Ding, Ning},
	month = may,
	year = {2025},
	note = {arXiv: 2505.22617},
}

@article{zheng_toward_2024,
	title = {Toward {Optimal} {LLM} {Alignments} {Using} {Two}-{Player} {Games}},
	url = {http://arxiv.org/abs/2406.10977},
	abstract = {The standard Reinforcement Learning from Human Feedback (RLHF) framework primarily focuses on optimizing the performance of large language models using pre-collected prompts. However, collecting prompts that provide comprehensive coverage is both tedious and challenging, and often fails to include scenarios that LLMs need to improve on the most. In this paper, we investigate alignment through the lens of two-agent games, involving iterative interactions between an adversarial and a defensive agent. The adversarial agent's task at each step is to generate prompts that expose the weakness of the defensive agent. In return, the defensive agent seeks to improve its responses to these newly identified prompts it struggled with, based on feedback from the reward model. We theoretically demonstrate that this iterative reinforcement learning optimization converges to a Nash Equilibrium for the game induced by the agents. Experimental results in safety scenarios demonstrate that learning in such a competitive environment not only fully trains agents but also leads to policies with enhanced generalization capabilities for both adversarial and defensive agents.},
	author = {Zheng, Rui and Guo, Hongyi and Liu, Zhihan and Zhang, Xiaoying and Yao, Yuanshun and Xu, Xiaojun and Wang, Zhaoran and Xi, Zhiheng and Gui, Tao and Zhang, Qi and Huang, Xuanjing and Li, Hang and Liu, Yang},
	month = jun,
	year = {2024},
	note = {arXiv: 2406.10977},
	keywords = {LLM, RL, alignment},
}

@misc{finn_connection_2016,
	title = {A {Connection} between {Generative} {Adversarial} {Networks}, {Inverse} {Reinforcement} {Learning}, and {Energy}-{Based} {Models}},
	url = {http://arxiv.org/abs/1611.03852},
	doi = {10.48550/arXiv.1611.03852},
	abstract = {Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the ﬁeld of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these ﬁelds, learning the cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at ﬁrst the connection between cost learning in RL and cost learning in generative modeling may appear to be a superﬁcial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator’s density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.},
	language = {en},
	urldate = {2025-08-26},
	publisher = {arXiv},
	author = {Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
	month = nov,
	year = {2016},
	note = {arXiv:1611.03852 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{eysenbach_maximum_2022,
	title = {Maximum {Entropy} {RL} ({Provably}) {Solves} {Some} {Robust} {RL} {Problems}},
	url = {http://arxiv.org/abs/2103.06257},
	abstract = {Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that maximum entropy (MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be used to learn policies that are robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL is a simple robust RL method with appealing formal guarantees.},
	author = {Eysenbach, Benjamin and Levine, Sergey},
	month = may,
	year = {2022},
	note = {arXiv: 2103.06257},
	keywords = {RL},
}

@misc{eysenbach_diversity_2018,
	title = {Diversity is {All} {You} {Need}: {Learning} {Skills} without a {Reward} {Function}},
	shorttitle = {Diversity is {All} {You} {Need}},
	url = {http://arxiv.org/abs/1802.06070},
	doi = {10.48550/arXiv.1802.06070},
	abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose “Diversity is All You Need”(DIAYN), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efﬁciency in reinforcement learning.},
	language = {en},
	urldate = {2025-08-26},
	publisher = {arXiv},
	author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
	month = oct,
	year = {2018},
	note = {arXiv:1802.06070 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@article{zhong_explaining_nodate,
	title = {Explaining {Datasets} in {Words}: {Statistical} {Models} with {Natural} {Language} {Parameters}},
	language = {en},
	author = {Zhong, Ruiqi and Wang, Heng and Klein, Dan and Steinhardt, Jacob},
}

@misc{fu_learning_2018,
	title = {Learning {Robust} {Rewards} with {Adversarial} {Inverse} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1710.11248},
	doi = {10.48550/arXiv.1710.11248},
	abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually speciﬁed reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difﬁcult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under signiﬁcant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.},
	language = {en},
	urldate = {2025-08-26},
	publisher = {arXiv},
	author = {Fu, Justin and Luo, Katie and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv:1710.11248 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{zhang_watermarks_2025,
	title = {Watermarks in the {Sand}: {Impossibility} of {Strong} {Watermarking} for {Generative} {Models}},
	shorttitle = {Watermarks in the {Sand}},
	url = {http://arxiv.org/abs/2311.04378},
	doi = {10.48550/arXiv.2311.04378},
	abstract = {Watermarking generative models consists of planting a statistical signal (watermark) in a model’s output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used.},
	language = {en},
	urldate = {2025-08-26},
	publisher = {arXiv},
	author = {Zhang, Hanlin and Edelman, Benjamin L. and Francati, Danilo and Venturi, Daniele and Ateniese, Giuseppe and Barak, Boaz},
	month = may,
	year = {2025},
	note = {arXiv:2311.04378 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{marks_auditing_2025,
  author       = {Samuel Marks and
                  Johannes Treutlein and
                  Trenton Bricken and
                  Jack Lindsey and
                  Jonathan Marcus and
                  Siddharth Mishra{-}Sharma and
                  Daniel M. Ziegler and
                  Emmanuel Ameisen and
                  Joshua Batson and
                  Tim Belonax and
                  Samuel R. Bowman and
                  Shan Carter and
                  Brian Chen and
                  Hoagy Cunningham and
                  Carson Denison and
                  Florian Dietz and
                  Satvik Golechha and
                  Akbir Khan and
                  Jan Kirchner and
                  Jan Leike and
                  Austin Meek and
                  Kei Nishimura{-}Gasparian and
                  Euan Ong and
                  Christopher Olah and
                  Adam Pearce and
                  Fabien Roger and
                  Jeanne Salle and
                  Andy Shih and
                  Meg Tong and
                  Drake Thomas and
                  Kelley Rivoire and
                  Adam S. Jermyn and
                  Monte MacDiarmid and
                  Tom Henighan and
                  Evan Hubinger},
  title        = {Auditing language models for hidden objectives},
  journal      = {CoRR},
  volume       = {abs/2503.10965},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2503.10965},
  doi          = {10.48550/ARXIV.2503.10965},
  eprinttype    = {arXiv},
  eprint       = {2503.10965},
  timestamp    = {Sun, 13 Apr 2025 20:59:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2503-10965.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sheshadri_latent_2025,
  author       = {Abhay Sheshadri and
                  Aidan Ewart and
                  Phillip Guo and
                  Aengus Lynch and
                  Cindy Wu and
                  Vivek Hebbar and
                  Henry Sleight and
                  Asa Cooper Stickland and
                  Ethan Perez and
                  Dylan Hadfield{-}Menell and
                  Stephen Casper},
  title        = {Latent Adversarial Training Improves Robustness to Persistent Harmful
                  Behaviors in LLMs},
  journal      = {Trans. Mach. Learn. Res.},
  volume       = {2025},
  year         = {2025},
  url          = {https://openreview.net/forum?id=6LxMeRlkWl},
  timestamp    = {Mon, 08 Sep 2025 20:51:58 +0200},
  biburl       = {https://dblp.org/rec/journals/tmlr/SheshadriEGLWHSSPHC25.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hendrycks_apps_2021,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}
@inproceedings{haarnoja_energybased_policies_2017,
  author       = {Tuomas Haarnoja and
                  Haoran Tang and
                  Pieter Abbeel and
                  Sergey Levine},
  editor       = {Doina Precup and
                  Yee Whye Teh},
  title        = {Reinforcement Learning with Deep Energy-Based Policies},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning,
                  {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series       = {Proceedings of Machine Learning Research},
  volume       = {70},
  pages        = {1352--1361},
  publisher    = {{PMLR}},
  year         = {2017},
  url          = {http://proceedings.mlr.press/v70/haarnoja17a.html},
  timestamp    = {Wed, 29 May 2019 08:41:45 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/HaarnojaTAL17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{haarnoja_softactor_2018,
  author       = {Tuomas Haarnoja and
                  Aurick Zhou and
                  Pieter Abbeel and
                  Sergey Levine},
  editor       = {Jennifer G. Dy and
                  Andreas Krause},
  title        = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
                  with a Stochastic Actor},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning,
                  {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
                  10-15, 2018},
  series       = {Proceedings of Machine Learning Research},
  volume       = {80},
  pages        = {1856--1865},
  publisher    = {{PMLR}},
  year         = {2018},
  url          = {http://proceedings.mlr.press/v80/haarnoja18b.html},
  timestamp    = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/HaarnojaZAL18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{hubinger_modelorganisms_2023,
title={Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research},
url={https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1},
journal={AI Alignment Forum},
author={Hubinger, Evan and Schiefer, Nicholas and Denison, Carson and Perez, Ethan},
year={2023},
month={Aug}} 

@article{deepseek_r1_2025,
  author       = {DeepSeek{-}AI and
                  Daya Guo and
                  Dejian Yang and
                  Haowei Zhang and
                  Junxiao Song and
                  Ruoyu Zhang and
                  Runxin Xu and
                  Qihao Zhu and
                  Shirong Ma and
                  Peiyi Wang and
                  Xiao Bi and
                  Xiaokang Zhang and
                  Xingkai Yu and
                  Yu Wu and
                  Z. F. Wu and
                  Zhibin Gou and
                  Zhihong Shao and
                  Zhuoshu Li and
                  Ziyi Gao and
                  Aixin Liu and
                  Bing Xue and
                  Bingxuan Wang and
                  Bochao Wu and
                  Bei Feng and
                  Chengda Lu and
                  Chenggang Zhao and
                  Chengqi Deng and
                  Chenyu Zhang and
                  Chong Ruan and
                  Damai Dai and
                  Deli Chen and
                  Dongjie Ji and
                  Erhang Li and
                  Fangyun Lin and
                  Fucong Dai and
                  Fuli Luo and
                  Guangbo Hao and
                  Guanting Chen and
                  Guowei Li and
                  H. Zhang and
                  Han Bao and
                  Hanwei Xu and
                  Haocheng Wang and
                  Honghui Ding and
                  Huajian Xin and
                  Huazuo Gao and
                  Hui Qu and
                  Hui Li and
                  Jianzhong Guo and
                  Jiashi Li and
                  Jiawei Wang and
                  Jingchang Chen and
                  Jingyang Yuan and
                  Junjie Qiu and
                  Junlong Li and
                  J. L. Cai and
                  Jiaqi Ni and
                  Jian Liang and
                  Jin Chen and
                  Kai Dong and
                  Kai Hu and
                  Kaige Gao and
                  Kang Guan and
                  Kexin Huang and
                  Kuai Yu and
                  Lean Wang and
                  Lecong Zhang and
                  Liang Zhao and
                  Litong Wang and
                  Liyue Zhang and
                  Lei Xu and
                  Leyi Xia and
                  Mingchuan Zhang and
                  Minghua Zhang and
                  Minghui Tang and
                  Meng Li and
                  Miaojun Wang and
                  Mingming Li and
                  Ning Tian and
                  Panpan Huang and
                  Peng Zhang and
                  Qiancheng Wang and
                  Qinyu Chen and
                  Qiushi Du and
                  Ruiqi Ge and
                  Ruisong Zhang and
                  Ruizhe Pan and
                  Runji Wang and
                  R. J. Chen and
                  R. L. Jin and
                  Ruyi Chen and
                  Shanghao Lu and
                  Shangyan Zhou and
                  Shanhuang Chen and
                  Shengfeng Ye and
                  Shiyu Wang and
                  Shuiping Yu and
                  Shunfeng Zhou and
                  Shuting Pan and
                  S. S. Li},
  title        = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement
                  Learning},
  journal      = {CoRR},
  volume       = {abs/2501.12948},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2501.12948},
  doi          = {10.48550/ARXIV.2501.12948},
  eprinttype    = {arXiv},
  eprint       = {2501.12948},
  timestamp    = {Wed, 06 Aug 2025 13:38:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2501-12948.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{vasan2024revisiting,
  title={Revisiting Constant Negative Rewards for Goal-Reaching Tasks in Robot Learning},
  author={Vasan, Gautham and Wang, Yan and Shahriar, Fahim and Bergstra, James and Jagersand, Martin and Rupam Mahmood, A},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}

@article{chen2024bootstrapping,
  title={Bootstrapping language models with dpo implicit rewards},
  author={Chen, Changyu and Liu, Zichen and Du, Chao and Pang, Tianyu and Liu, Qian and Sinha, Arunesh and Varakantham, Pradeep and Lin, Min},
  journal={arXiv preprint arXiv:2406.09760},
  year={2024}
}

@article{su2025thinking,
  title={Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards},
  author={Su, Jinyan and Cardie, Claire},
  journal={arXiv preprint arXiv:2505.18298},
  year={2025}
}