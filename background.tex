\section{Background} \label{sec:background}

\subsection{Underdeterminacy Exploitation}

I (Xanthe) think that we need to carefully disambiguate how our problem setting is different to reward hacking, probably here... or do we? hmm

Important point: a task can in theory be completely specified yet underdetermined, so not susceptible to reward hacking (in the underspecification sense that is usually intended by this), but susceptible to underdeterminacy exploitation. Many (most?) tasks we are actually interested in solving are both underspecified and underdetermined.



\subsection{Problem setting: formalising}

Consider an input space $X$ and output space $Y$. For a primary objective with reward $R:X\times Y \to [0,1]$, we model the exploitable search problem by evaluating an agent jointly on a malicious unspecified objective (see below) and the primary reward, $R$. Given a distribution $\mu$ over $X$, we then have average reward $\mathbb{E}_{x\sim\mu}R(x, A(x))$. Let $A^*$ be an agent obtaining near optimal average reward. Now assume that there is a secret reward function $S:X \times Y\to [0,1]$ which $A$ is optimising for. The exploitable search problem is to prevent the optimisation of $S$ whilst still achieving similar expected reward to $A^*$.

\subsection{Model organisms}

% Like many alignment failure modes, underdeterminacy exploitation 
To study underdeterminacy exploitation, we develop LLM variants which pursue unspecified objectives concurrently to the objective specified by its user. 
% below should be left to methods -- Xanthe
%The unspecified objective description and harness are hidden from downstream mitigation methods. 
%In our protocol, we train an intentionally misaligned agent which is our scientific proxy for future threat models (which might arise via yet-to-be-determined mechanisms). 
Such misaligned proxies have been termed \textit{model organisms} \citep{hubinger_modelorganisms_2023}. Initially we wish to achieve high performance on a main objective. During this process, an unspecified objective is introduced, perhaps due to a spurious signal or correlation. The resulting model organism will pursue an unspecified objective across environments. For our purposes, model organisms are developed via prompting, training or steering. We wish to remove this behaviour through a form of mitigation training.       

% Our specified objective is defined as successfully completing a task specified by the user. For underspecified tasks this may be one of many solutions. A unspecified objective is defined as an additional objective which may not directly relate to providing a correct solution. While this need not necessarily be orthogonal to the primary objective, in this paper we only concern ourselves with orthogonal unspecified objectives\footnote{An unspecified objective is considered orthogonal to the main objective if optimising for the unspecified objective has little-to-no effect on optimising for the main objective. For example, a unspecified objective of leaving as many comments as possible does not affect the main objective: writing correct code.}.


%Now assume that $A^*$ also has one of the following two goals.
 
% \begin{enumerate}
    % \item There is a secret reward function $S:X \times Y\to [0,1]$ which $A^*$ would like to optimise.
    % \item Let $z$ be additional bits independent of $x$ that $A^*$ would like to covertly encode. $A^*$ then wishes to use $y=A(x, z)$ as an information channel, such that the Shannon channel capacity from $z\to y$ is high.  
% \end{enumerate}

% Note our misalignment assumption is a corollary of an information theoretic definition which does not require the definition of $S$. The exploitable search problem aims to prevent 1) or 2) whilst still achieving similar expected reward to $A^*$.

\subsubsection{Reinforcement Learning Framework}
In the standard reinforcement learning (RL) setup, an agent interacts with an environment by sampling actions $a_t$ from a policy $\pi(a|s_t)$ conditional on current state, with the objective of maximising an expected cumulative reward:

\begin{equation}
    \pi^* = \arg \underset{\pi}{\max}\ \mathbb{E}_\pi
    \left[\sum^T_{t=0}\gamma^t r(s_t, a_t)\right],
\end{equation}

where $r(s_t, a_t)$ denotes the reward at time $t$, $T$ denotes the total trajectory length and $\gamma$ is a discount factor. For Large Language Models (LLMs), $\pi(a|s_t)$ represents the model, $s_t$ and $a_t$ are the context window and the token generated at time $t$ respectively. This objective serves as the default on top of which modifications can be introduced to induce stochasticity, improve robustness, or prevent undesirable optimisation dynamics.

\subsection{Existing approaches}

The exploitable search problem has not been directly addressed in previous work, but methods controlling the diversity of learned policies have been proposed for other purposes. We will consider approaches that modify the learning objective by adding an additional reward, denoted here as $R_{mit}$

\begin{equation}
    \pi^* = \arg \underset{\pi}{\max}\;\mathbb{E}_\pi \left[\sum^T_{t=0}\gamma^t r(s_t, a_t) + R_{mit}\right].
\end{equation}

%These approaches were originally developed for broader RL challenges such as improving exploration, or reducing training costs. 


\paragraph{Maximum entropy reinforcement learning}

Maximum entropy (MaxEnt) RL augments the reward with an entropy term, such that the optimal policy aims to maximize its entropy at each visited state: \citep{haarnoja_energybased_policies_2017},

\begin{equation}
     R_{mit} = \alpha\mathcal{H}(\pi(\cdot|s_t)).
\end{equation}

Here $\alpha$ is the entropy penalty coefficient hyperparameter and $\mathcal{H}(\pi(\cdot|s_t)) = -\sum_a\pi(a|s_t)\log\pi(a|s_t)$ is the
entropy of the policy at state $s_t$. MaxEnt RL has been shown both in theory and practice to induce policies resilient to environment disturbances \cite{eysenbach_diversity_2018}, as well as enhance exploration during training \citep{cui_entropy_2025}. By encouraging diverse trajectories, MaxEnt RL may reduce the consistency of patterns in policy behaviour that do not affect the primary reward, providing a potential mechanism for mitigating undesired regularities.
% \begin{itemize}
%     \item reference soft actor critic paper? - off policy max ent rl - \citep{haarnoja_softactor_2018} has all the references for background in mat ent
% \end{itemize}

% Eysenbach and Levine \citep{eysenbach_maximum_2022} show theoretically that training via maximum entropy (MaxEnt) RL  induces policies resilient to disturbances in state-transition dynamics or rewards. Separately, recent work on policy entropy collapse during RL on LLMs demonstrates that entropy penalties can improve performance by preventing entropy collapse early in training that would otherwise undermine exploration and limit reward \citep{cui_entropy_2025}. 



%It is found that entropy management is necessary for scaling RL on LLMs. This provides a natural basis for the application of maximum entropy as a mitigation strategy. 

\paragraph{Length penalties}
Instead of increasing diversity of solutions generated, we may instead limit the set of high-reward solutions. The degrees of freedom left to be exploited for an unspecified objective would then also be constrained. A typical option used for inference cost reduction is length penalties:

\begin{equation}
    R_{mit} =  - \lambda\cdot\mathrm{T}.
\end{equation}

In this setting, $\lambda$ is a hyperparameter, and $T$ is the trajectory length, measured as number of generated tokens for a given completion.
% Another traditional method is the use of a length penalty during RL. A length penalty discourages overly long sequences by penalising the total score of a trajectory based on length. When added at the reward level, the formula is typically of the form $R-\lambda \cdot\mathrm{length}$ [ref?]. Length penalties can indirectly cause a model to randomise more in action space, especially during early steps, because the pressure to be efficient leads to broader exploration of possible actions.

\subsubsection{Adversarial games}

A two player adversarial game might be able to provide the necessary randomisation without being computationally prohibitive. Two player games have been shown to handle high dimensional trajectory spaces remarkably well \citep{finn_connection_2016,fu_learning_2018}. Furthermore, such adversarial style set ups have been shown to be equivalent to traditional entropy methods in certain contexts. A typical game involves an adversarial set up between two players Alice and Bob. Each agent is tasked with optimising conflicting rewards in an iterative process. The key concept underlying the potential of the 2 player method is that, given a suitable critic, we can perform a stochastic walk in space for an output with desired properties. This has been demonstrated in Watermarks in the sand \citep{zhang_watermarks_2025}. In our application we aim to perform such randomisation that no distinct malicious region of solution space can be persistently optimised for by an agent.

% \subsection{Missing topics and papers}

% \begin{itemize}
%     \item add injecting noise and fuzzing method? - or show equivalent to max entropy
%     \item increasing temperature 
%     \item "Activation/latent-space-noise baseline is something to consider as well (Fabien Roger)"
%     \item fuzzing less wrong post (related work?) - https://www.lesswrong.com/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets
% \end{itemize}

% Original papers from jacob:
% A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models
% learning robust rewards with adversarial inverse reinforcement learning
% Toward Optimal LLM Alignments Using Two-Player Games
% https://arxiv.org/pdf/2505.22617
% Explaining Datasets in Words: Statistical Models with Natural Language Parameters
% https://arxiv.org/pdf/2311.04378
% [2103.06257] Maximum Entropy RL (Provably) Solves Some Robust RL Problems
% Diversity is All You Need: Learning Skills without a Reward Function

